{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:43: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#0.823\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from operator import itemgetter\n",
    "import zipfile\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "random.seed(2016)\n",
    "\n",
    "\n",
    "def create_feature_map(features):\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    for i, feat in enumerate(features):\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "def get_importance(gbm, features):\n",
    "    create_feature_map(features)\n",
    "    importance = gbm.get_fscore(fmap='xgb.fmap')\n",
    "    importance = sorted(importance.items(), key=itemgetter(1), reverse=True)\n",
    "    return importance\n",
    "\n",
    "\n",
    "def intersect(a, b):\n",
    "    return list(set(a) & set(b))\n",
    "\n",
    "\n",
    "def print_features_importance(imp):\n",
    "    for i in range(len(imp)):\n",
    "        print(\"# \" + str(imp[i][1]))\n",
    "        print('output.remove(\\'' + imp[i][0] + '\\')')\n",
    "\n",
    "\n",
    "def run_default_test(train, test, features, target, random_state=0):\n",
    "    eta = 0.1\n",
    "    max_depth = 7\n",
    "    subsample = 0.8\n",
    "    colsample_bytree = 0.8\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('XGBoost params. ETA: {}, MAX_DEPTH: {}, SUBSAMPLE: {}, COLSAMPLE_BY_TREE: {}'.format(eta, max_depth, subsample, colsample_bytree))\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"eta\": eta,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\": 1,\n",
    "        \"seed\": random_state\n",
    "    }\n",
    "    num_boost_round = 1000\n",
    "    early_stopping_rounds = 20\n",
    "    test_size = 0.1\n",
    "\n",
    "    X_train, X_valid = train_test_split(train, test_size=test_size, random_state=random_state)\n",
    "    y_train = X_train[target]\n",
    "    y_valid = X_valid[target]\n",
    "    dtrain = xgb.DMatrix(X_train[features], y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid[features], y_valid)\n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
    "\n",
    "    print(\"Validating...\")\n",
    "    check = gbm.predict(xgb.DMatrix(X_valid[features]), ntree_limit=gbm.best_ntree_limit)\n",
    "    score = roc_auc_score(X_valid[target].values, check)\n",
    "    print('Check error value: {:.6f}'.format(score))\n",
    "\n",
    "    imp = get_importance(gbm, features)\n",
    "    print('Importance array: ', imp)\n",
    "\n",
    "    print(\"Predict test set...\")\n",
    "    test_prediction = gbm.predict(xgb.DMatrix(test[features]), ntree_limit=gbm.best_ntree_limit)\n",
    "\n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "    return test_prediction.tolist(), score\n",
    "\n",
    "\n",
    "def create_submission(score, test, prediction):\n",
    "    # Make Submission\n",
    "    now = datetime.datetime.now()\n",
    "    sub_file = 'submission_' + str(score) + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "    print('Writing submission: ', sub_file)\n",
    "    f = open(sub_file, 'w')\n",
    "    f.write('id,probability\\n')\n",
    "    total = 0\n",
    "    for id in test['id']:\n",
    "        str1 = str(id) + ',' + str(prediction[total])\n",
    "        str1 += '\\n'\n",
    "        total += 1\n",
    "        f.write(str1)\n",
    "    f.close()\n",
    "\n",
    "    # print('Creating zip-file...')\n",
    "    # z = zipfile.ZipFile(sub_file + \".zip\", \"w\", zipfile.ZIP_DEFLATED)\n",
    "    # z.write(sub_file)\n",
    "    # z.close()\n",
    "\n",
    "\n",
    "def get_features(train, test):\n",
    "    trainval = list(train.columns.values)\n",
    "    testval = list(test.columns.values)\n",
    "    output = intersect(trainval, testval)\n",
    "    #output.remove('itemID_1')\n",
    "    #output.remove('itemID_2')\n",
    "    return output\n",
    "\n",
    "\n",
    "def prep_dataset(is_train, dataset_pairs_path, dataset_info_path):\n",
    "    start_time = time.time()\n",
    "\n",
    "    types1 = {\n",
    "        'itemID_1': np.dtype(int),\n",
    "        'itemID_2': np.dtype(int),\n",
    "        'isDuplicate': np.dtype(int),\n",
    "        'generationMethod': np.dtype(int),\n",
    "    }\n",
    "\n",
    "    types2 = {\n",
    "        'itemID': np.dtype(int),\n",
    "        'categoryID': np.dtype(int),\n",
    "        'title': np.dtype(str),\n",
    "        'description': np.dtype(str),\n",
    "        'images_array': np.dtype(str),\n",
    "        'attrsJSON': np.dtype(str),\n",
    "        'price': np.dtype(float),\n",
    "        'locationID': np.dtype(int),\n",
    "        'metroID': np.dtype(float),\n",
    "        'lat': np.dtype(float),\n",
    "        'lon': np.dtype(float),\n",
    "    }\n",
    "\n",
    "    pairs = pd.read_csv(dataset_pairs_path, dtype=types1)\n",
    "\n",
    "    # Add 'id' column for easy merge\n",
    "    items = pd.read_csv(dataset_info_path, dtype=types2)\n",
    "    items.fillna(-1, inplace=True)\n",
    "    location = pd.read_csv(\"input/Location.csv\")\n",
    "    category = pd.read_csv(\"input/Category.csv\")\n",
    "\n",
    "    train = pairs\n",
    "    \n",
    "    if is_train:\n",
    "        train = train.drop(['generationMethod'], axis=1)\n",
    "\n",
    "    print('Add text features...')\n",
    "    train['len_title'] = items['title'].str.len()\n",
    "    train['len_description'] = items['description'].str.len()\n",
    "    train['len_attrsJSON'] = items['attrsJSON'].str.len()\n",
    "\n",
    "    print('Merge item 1...')\n",
    "    item1 = items[['itemID', 'categoryID', 'description', 'price', 'locationID', 'metroID', 'lat', 'lon']]\n",
    "    item1 = pd.merge(item1, category, how='left', on='categoryID', left_index=True)\n",
    "    item1 = pd.merge(item1, location, how='left', on='locationID', left_index=True)\n",
    "\n",
    "    item1 = item1.rename(\n",
    "        columns={\n",
    "            'itemID': 'itemID_1',\n",
    "            'categoryID': 'categoryID_1',\n",
    "            'description': 'description_1',\n",
    "            'parentCategoryID': 'parentCategoryID_1',\n",
    "            'price': 'price_1',\n",
    "            'locationID': 'locationID_1',\n",
    "            'regionID': 'regionID_1',\n",
    "            'metroID': 'metroID_1',\n",
    "            'lat': 'lat_1',\n",
    "            'lon': 'lon_1'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add item 1 data\n",
    "    train = pd.merge(train, item1, how='left', on='itemID_1', left_index=True)\n",
    "\n",
    "    print('Merge item 2...')\n",
    "    item2 = items[['itemID', 'categoryID', 'description', 'price', 'locationID', 'metroID', 'lat', 'lon']]\n",
    "    item2 = pd.merge(item2, category, how='left', on='categoryID', left_index=True)\n",
    "    item2 = pd.merge(item2, location, how='left', on='locationID', left_index=True)\n",
    "\n",
    "    item2 = item2.rename(\n",
    "        columns={\n",
    "            'itemID': 'itemID_2',\n",
    "            'categoryID': 'categoryID_2',\n",
    "            'description': 'description_2',\n",
    "            'parentCategoryID': 'parentCategoryID_2',\n",
    "            'price': 'price_2',\n",
    "            'locationID': 'locationID_2',\n",
    "            'regionID': 'regionID_2',\n",
    "            'metroID': 'metroID_2',\n",
    "            'lat': 'lat_2',\n",
    "            'lon': 'lon_2'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add item 2 data\n",
    "    train = pd.merge(train, item2, how='left', on='itemID_2', left_index=True)\n",
    "\n",
    "    # Create same arrays\n",
    "    print('Create same arrays')\n",
    "    train['price_same'] = np.equal(train['price_1'], train['price_2']).astype(np.int32)\n",
    "    train['locationID_same'] = np.equal(train['locationID_1'], train['locationID_2']).astype(np.int32)\n",
    "    train['categoryID_same'] = np.equal(train['categoryID_1'], train['categoryID_2']).astype(np.int32)\n",
    "    train['regionID_same'] = np.equal(train['regionID_1'], train['regionID_2']).astype(np.int32)\n",
    "    train['metroID_same'] = np.equal(train['metroID_1'], train['metroID_2']).astype(np.int32)\n",
    "    train['lat_same'] = np.equal(train['lat_1'], train['lat_2']).astype(np.int32)\n",
    "    train['lon_same'] = np.equal(train['lon_1'], train['lon_2']).astype(np.int32)\n",
    "\n",
    "    #tfidf = TfidfVectorizer(stop_words = stopwords.words('russian')).fit_transform([train['description_1'].str, train['description_2'].str])\n",
    "    #cosine_similarities = linear_kernel(tfidf[0], tfidf[1]).flatten()    \n",
    "    #train['description_diff'] = cosine_similarities[0]\n",
    "\n",
    "    train['description_1'].apply(len)# = len(train['description_1'])\n",
    "    train['description_2'].apply(len)# = len(train['description_2'])\n",
    "\n",
    "    \n",
    "    print('Create train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    train.drop(['categoryID_1', \n",
    "                'categoryID_2', \n",
    "                'categoryID_same', \n",
    "                'description_1', \n",
    "                'description_2', \n",
    "                'price_1', \n",
    "                'price_2', \n",
    "                'lon_1', \n",
    "                'lon_2',\n",
    "                'parentCategoryID_1', \n",
    "                'parentCategoryID_2', \n",
    "                'price_same', \n",
    "                'regionID_1', \n",
    "                'regionID_2', \n",
    "                'regionID_same'], axis=1, inplace=True)\n",
    "    return train\n",
    "\n",
    "def read_test_train():\n",
    "    train = prep_dataset(True, \"input/ItemPairs_train.csv\", \"input/ItemInfo_train.csv\")\n",
    "    train.to_csv('input/train_merged.csv')\n",
    "    test = prep_dataset(False, \"input/ItemPairs_test.csv\", \"input/ItemInfo_test.csv\")\n",
    "    test.to_csv('input/test_merged.csv')\n",
    "    \n",
    "    train.fillna(-1, inplace=True)\n",
    "    test.fillna(-1, inplace=True)\n",
    "    # Get only subset of data\n",
    "    if 1:\n",
    "        len_old = len(train.index)\n",
    "        train = train.sample(frac=0.01)\n",
    "        len_new = len(train.index)\n",
    "        print('Reduce train from {} to {}'.format(len_old, len_new))\n",
    "    features = get_features(train, test)\n",
    "    return train, test, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add text features...\n",
      "Merge item 1...\n",
      "Merge item 2...\n",
      "Create same arrays\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1cbfca1459f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_test_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Length of train: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Length of test: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Features [{}]: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_default_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'isDuplicate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-9a96a0a09738>\u001b[0m in \u001b[0;36mread_test_train\u001b[1;34m()\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_test_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"input/ItemPairs_train.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"input/ItemInfo_train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'input/train_merged.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"input/ItemPairs_test.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"input/ItemInfo_test.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-9a96a0a09738>\u001b[0m in \u001b[0;36mprep_dataset\u001b[1;34m(is_train, dataset_pairs_path, dataset_info_path)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;31m#train['description_diff'] = cosine_similarities[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description_1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# = len(train['description_1'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# = len(train['description_2'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas-0.18.0-py2.7-linux-x86_64.egg/pandas/core/series.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2235\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboxer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2237\u001b[1;33m         \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2238\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2239\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:63043)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "train, test, features = read_test_train()\n",
    "print('Length of train: ', len(train))\n",
    "print('Length of test: ', len(test))\n",
    "print('Features [{}]: {}'.format(len(features), sorted(features)))\n",
    "test_prediction, score = run_default_test(train, test, features, 'isDuplicate')\n",
    "print('Real score = {}'.format(score))\n",
    "create_submission(score, test, test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
