{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do zaimplementowania kiedyś"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mixowanie modeli\n",
    "* Użycie lasów losowych ( powinny się dobrze sprawdzić przy takich danych )\n",
    "\n",
    "* wordnet.ru\n",
    "* WordNetLemmatizer\n",
    "* użyć TF-IDF do kategorii z JSONów (niedługo się tym zajmę). Tf-IDFY mogą być za ciężkie, być może wystarczy zliczenie identycznych kategorii\n",
    "* zbadać przechodność relacji duplikacji. Czy uzupełnienie zbioru treningowego da nam coś? https://www.kaggle.com/c/avito-duplicate-ads-detection/forums/t/20892/transitive-relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Użycie większej liczby cech z obrazków (poza kategoriami wyuczonymi 17 maja).\n",
    "\n",
    "https://pypi.python.org/pypi/PhotoHash\n",
    "\n",
    "https://pypi.python.org/pypi/ImageHash\n",
    "\n",
    "http://blog.iconfinder.com/detecting-duplicate-images-using-python/\n",
    "\n",
    "http://www.hackerfactor.com/blog/?/archives/432-Looks-Like-It.html\n",
    "\n",
    "https://www.pureftpd.org/project/libpuzzle\n",
    "\n",
    "Nie używać pHash, bo jest komercyjny i nie wolno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nauczenie się konwolucyjnej sieci do obrazków od podstaw (albo zbudowanie sieci, która zastąpi boosting\n",
    "\n",
    "Z forum:\n",
    "> Probably none of the submissions used the images yet, since they are still far below the benchmark.. That's understandable due to the size of the dataset. You may want to look into past competitions that involved computer vision. The usual approach is to use a conv neural net to extract features from images in the form of a vector, and then feed the vector into a classification model of your choice. That may require a GPU, but can be done with CPU only, though it takes a lot longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ogarnąć wyniki AUC https://www.kaggle.com/c/santander-customer-satisfaction/forums/t/20725/kaggle-auc-sklearn-auc-was-a-factor-in-lb-shake-up\n",
    "\n",
    "Duża różnica oznacza, że rozkład danych na zbiorze testowym jest inny niż na zbiorze treningowym. W takim razie poza wynikami\n",
    "na CV trzeba będzie też używać wyników z LB aby ocenić, które rozwiązanie jest najlepsze (mimo, że normalnie nie wolno\n",
    "tego robić - patrz \"how to overfit leaderboard\")\n",
    "\n",
    "Warto zauważyć, że niektóre cechy będą miały ten sam rozkład, niektóre zupełnie inny. Należy przeprowadzić analizę\n",
    "statystyczną i porysować wykresiki.\n",
    "\n",
    "Wygląda na to, że dane są podzielone wg dat (z forum):\n",
    "    \n",
    "> Hi there! I found out that shuffling is very important. Here are my results from logistic regression on 4 very simple features. First, results from 5-fold stratified split (from sklearn) without shuffling:\n",
    ">\n",
    ">    AUC: 0.7661\n",
    ">    AUC: 0.7805\n",
    ">    AUC: 0.7343\n",
    ">    AUC: 0.6626\n",
    ">    AUC: 0.6555\n",
    ">\n",
    "> And results with shuffling:\n",
    ">\n",
    ">    AUC: 0.7184\n",
    ">    AUC: 0.7179\n",
    ">    AUC: 0.7185\n",
    ">    AUC: 0.7186\n",
    ">    AUC: 0.7193\n",
    ">\n",
    "> You can see, that with shuffling, results are much more stable and reliable. It's clear evidence that the data has a time structure, so I suppose you should split the data not randomly. First N% rows for traininig, remaining are for validation. It's also clear because of decrease of AUC with moving validation window further to the end of the data.\n",
    "\n",
    "Tu wypowiedź kolesia, ktory zajmuje pierwsze miejsce\n",
    "\n",
    "> I've found consistently in my testing that as you train more XGBoost rounds, the delta between leaderboard score and local CV increases, even if the deviation locally is very low and difference between train/cv auc is very low.\n",
    ">\n",
    "> I suspect this might be because the training and testing sets are from different dates, and so the content differs somewhat. Looking at the values in the dataset, there does seem to be quite a large difference between training/testing sets, while the data is very uniform within the training set, so this could be what is causing the difference between LB/CV scores.\n",
    ">\n",
    "> Just for reference, my current scores are\n",
    ">\n",
    "> CV: ~0.945 LB: ~0.905\n",
    "\n",
    "Oznacza to, że warto będzie spróbować specjalnie delikatnie underfitować\n",
    "\n",
    "> I made my first sub today using a very simple model (no attrsJSON/no images/no cat and location files/default xgb params) just to see how it would fare in LB. Got 0.91x in local validation and 0.81x in LB. I wonder if I'm overfitting or if when I start adding more features that difference will shrink. \n",
    "\n",
    "I jeszcze jedna wypowiedź\n",
    "\n",
    "> It seems like some of duplicates presented by more that one pair in training set, and some pair can get in train and some - in test set with random partitioning, so classifier learn to recognize concrete ads instead of really classify duplicates, so with increase of tree depth CV score increases, but LB don't increases (of course, these concrete ads didn't presented in LB dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kolejnym ciekawym pomysłem jest zmiana rozkładu zbioru treningowego tak, aby dopasował się do testowego. Dodatko|wo mix\n",
    "modeli na danych orginalnych i zmienionych może być mocny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dodanie cech relatywnego dystansu - np. dla ceny abs(cena1 - cena2) / (cena1 + cena2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ważne (do załatwienia na wczoraj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Jeśli chcemy inaczej dzielić zbiory na treningowy i walidację trzeba jak najszybciej to zmienić, aby benchmark na CV był uczciwy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| ID | Opis metody        | CV           | LB  |\n",
    "| -- | ------------- |:-------------:| -----:|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
